
下载dump：https://dumps.wikimedia.org/zhwiki/latest/，

维基数据主要包含以下几部分：　　　     
zhwiki-latest-pages-articles.xml.bz2  词条正文
zhwiki-latest-redirect.sql 	          词条重定向（同义词）
zhwiki-latest-pagelinks.sql 	        词条页面内容外链
zhwiki-latest-page.sql	              词条标题及摘要
zhwiki-latest-categorylinks.sql 	    词条开放分类链接


数据的抽取
Gensim是一个相当专业的主题模型Python工具包，提供了wiki数据的抽取处理类WikiCorpus，
能对下载的数据（*articles.xml.bz2）进行抽取处理，得到纯净的文本语料。

创建process_wiki_1.py
# -*- coding: utf-8 -*-
import logging
import sys
from gensim.corpora import WikiCorpus
logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s', level=logging.INFO)
'''
    extract data from wiki dumps(*articles.xml.bz2) by gensim.
    @chenbingjin 2016-05-11
'''
def help():
    print "Usage: python process_wiki.py zhwiki-latest-pages-articles.xml.bz2 wiki.zh.txt"

if __name__ == '__main__':
    if len(sys.argv) < 3:
        help()
        sys.exit(1)
    logging.info("running %s" % ' '.join(sys.argv))
    inp, outp = sys.argv[1:3]
    i = 0

    output = open(outp, 'w')
    wiki = WikiCorpus(inp, lemmatize=False, dictionary={})
    for text in wiki.get_texts():
        output.write(" ".join(text) + "\n")
        i = i + 1
        if (i % 10000 == 0):
            logging.info("Save "+str(i) + " articles")
    output.close()
    logging.info("Finished saved "+str(i) + "articles")

数据预处理
    繁体转简体：使用的是开源简繁转换工具OpenCC
    字符编码转换：使用iconv命令将文件转换成utf-8编码：iconv -c -t UTF-8 < input_file > output_file
    分词处理：使用jieba分词工具包，命令行分词：python -m jieba input_file > cut_file

创建process_wiki_2.sh
#!/bin/bash

# preprocess data

# Traditional Chinese to Simplified Chinese
echo "opencc: Traditional Chinese to Simplified Chinese..."
#time opencc -i wiki.zh.txt -o wiki.zh.chs.txt -c zht2zhs.ini
time opencc -i wiki.zh.txt -o wiki.zh.chs.txt -c t2s.json

# Cut words
echo "jieba: Cut words..."
time python -m jieba -d ' ' wiki.zh.chs.txt > wiki.zh.seg.txt

# Change encode
echo "iconv: ascii to utf-8..."
time iconv -c -t UTF-8 < wiki.zh.seg.txt > wiki.zh.seg.utf.txt
