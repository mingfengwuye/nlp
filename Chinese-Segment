由于英文采用空格进行分词，采用nltk进行token时，需先将中文进行处理。

jieba
##############################################################################
#!/usr/bin/python
#-- encoding:utf-8 --
import jieba #导入jieba模块

def splitSentence(inputFile, outputFile):
fin = open(inputFile, 'r') #以读的方式打开文件
fout = open(outputFile, 'w') #以写得方式打开文件

for eachLine in fin:  
    line = eachLine.strip().decode('utf-8', 'ignore')       #去除每行首尾可能出现的空格，并转为Unicode进行处理  
    wordList = list(jieba.cut(line))                        #用结巴分词，对每行内容进行分词  
    outStr = ''  
    for word in wordList:  
        outStr += word  
        outStr += '/ '  
    fout.write(outStr.strip().encode('utf-8') + '\n')       #将分词好的结果写入到输出文件  
fin.close()  
fout.close()  
splitSentence('myInput.txt', 'myOutput.txt')

stanford-segmenter
#############################################################################
Download stanford-segmenter https://nlp.stanford.edu/software/segmenter.shtml

./segment.sh pku test.simp.utf8 UTF-8 0 > path/to/segmented.file


THULAC
##############################################################################
THULAC（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。
https://github.com/thunlp/THULAC
